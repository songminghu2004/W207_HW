{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named lasagne",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6f01c87e5bba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnesterov_momentum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnolearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlasagne\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named lasagne"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pandas.io.parsers import read_csv\n",
    "from datetime import datetime\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "import theano\n",
    "\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other Functions or Classes\n",
    "# Function to load train/test csv files \n",
    "def load(test=False, cols=None):\n",
    "    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "    Pass a list of *cols* if you're only interested in a subset of the\n",
    "    target columns.\n",
    "    \"\"\"\n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "\n",
    "    if cols:  # get a subset of columns\n",
    "        df = df[list(cols) + ['Image']]\n",
    "\n",
    "    print(df.count())  # prints the number of values for each column\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    if not test:  # only FTRAIN has any target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Class to display list as HTML Table in Jupyter Notebook\n",
    "class ListTable(list):\n",
    "    \"\"\" Overridden list class which takes a 2-dimensional list of \n",
    "        the form [[1,2,3],[4,5,6]], and renders an HTML Table in \n",
    "        IPython Notebook. \"\"\"\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "def plot_sample(x, y, axis):\n",
    "    img = x.reshape(96, 96)\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=30, c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FTRAIN = '/data/facial_keypoints_detection/training.csv'\n",
    "FTEST = '/data/facial_keypoints_detection/test.csv'\n",
    "\n",
    "# Load Test data\n",
    "Test_X, _ = load(test=True)\n",
    "\n",
    "# Load Train data\n",
    "X, y = load()\n",
    "print(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n",
    "    X.shape, X.min(), X.max()))\n",
    "print(\"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\".format(\n",
    "    y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list of hiddern unit number\n",
    "hidden_num_units = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "# Define a listtable to hold statsitical outcomes of NNs with various hiddern unit numbers \n",
    "performance_table_1 = ListTable()\n",
    "performance_table_1.append(['Num_hidden_layer', 'Train_loss', 'Validation_loss', 'Duration_time(s)'])\n",
    "\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "for index,num in enumerate(hidden_num_units):\n",
    "    net1 = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                             ('hidden', layers.DenseLayer),\n",
    "                             ('output', layers.DenseLayer),],\n",
    "                     input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                     #hidden_num_units=200,  # number of units in hidden layer\n",
    "                     output_nonlinearity=None,  # output layer uses identity function\n",
    "                     output_num_units=30,  # 30 target values\n",
    "                     \n",
    "                     update=nesterov_momentum, # optimization parameters\n",
    "                     update_learning_rate=0.01,\n",
    "                     update_momentum=0.9,\n",
    "                     \n",
    "                     regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                     max_epochs=1000,  # we want to train this many epochs\n",
    "                     verbose=0,)\n",
    "    net1.hidden_num_units = num\n",
    "    net1.fit(X, y)\n",
    "    \n",
    "    train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "    valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "    print net1.hidden_num_units \n",
    "    print \"Train, valid errors and computational time of last epcho: %0.4f, %0.4f, %0.4f\" %(net1.train_history_[-1][\"train_loss\"], net1.train_history_[-1][\"valid_loss\"], net1.train_history_[-1][\"dur\"])\n",
    "    performance_table_1.append([net1.hidden_num_units,\n",
    "                                round(net1.train_history_[-1][\"train_loss\"],5),\n",
    "                                round(net1.train_history_[-1][\"valid_loss\"],5),\n",
    "                                round(net1.train_history_[-1][\"dur\"],5)] )\n",
    "    \n",
    "    subfig = fig.add_subplot(2, 5, index+1, xticks=[], yticks=[])\n",
    "    plt.plot(train_loss, linewidth=3, color=\"blue\", linestyle=\"dashed\", label=\"train\")\n",
    "    plt.plot(valid_loss, linewidth=3, color=\"blue\", linestyle=\"solid\", label=\"valid\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.ylim(5*1e-4, 1e-2)\n",
    "    plt.xlim(0,1000)\n",
    "    plt.yscale(\"log\")\n",
    "    subfig.set_title('Num_Hidden_Layer: ' + str(num))\n",
    "\n",
    "plt.show()\n",
    "performance_table_1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net1 = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                         ('hidden', layers.DenseLayer),\n",
    "                         ('output', layers.DenseLayer),],\n",
    "                 input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                 hidden_num_units=500,  # number of units in hidden layer\n",
    "                 output_nonlinearity=None,  # output layer uses identity function\n",
    "                 output_num_units=30,  # 30 target values\n",
    "                 \n",
    "                 update=nesterov_momentum, # optimization parameters\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                 max_epochs=1000,  # we want to train this many epochs\n",
    "                 verbose=0,)\n",
    "\n",
    "net1.fit(X, y)\n",
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1.train_history_[-1][\"train_loss\"], net1.train_history_[-1][\"valid_loss\"], net1.train_history_[-1][\"dur\"])\n",
    "\n",
    "Test_y_pred = net1.predict(Test_X)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(40):\n",
    "    ax = fig.add_subplot(8, 5, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(Test_X[i], Test_y_pred[i], ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_stretch = np.empty_like(X)\n",
    "X_eq = np.empty_like(X)\n",
    "X_adapteq = np.empty_like(X)\n",
    "\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    img = X[i,]\n",
    "    p_lowbound, p_upperbound = np.percentile(img, (1, 99))\n",
    "    X_stretch[i,] = exposure.rescale_intensity(img, in_range=(p_lowbound,p_upperbound ))\n",
    "    \n",
    "for i in range(X.shape[0]):\n",
    "    img = X[i,]\n",
    "    X_eq[i,] =  exposure.equalize_hist(img,nbins=255)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    img = X[i,].reshape(96,96)\n",
    "    X_adapteq[i,] =  exposure.equalize_adapthist(img, clip_limit=0.03).flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_adapteq.shape\n",
    "print X_adapteq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net1_stretch = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                         ('hidden', layers.DenseLayer),\n",
    "                         ('output', layers.DenseLayer),],\n",
    "                 input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                 hidden_num_units=500,  # number of units in hidden layer\n",
    "                 output_nonlinearity=None,  # output layer uses identity function\n",
    "                 output_num_units=30,  # 30 target values\n",
    "                 \n",
    "                 update=nesterov_momentum, # optimization parameters\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                 max_epochs=1000,  # we want to train this many epochs\n",
    "                 verbose=0,)\n",
    "\n",
    "net1_stretch.fit(X_stretch, y)\n",
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1_stretch.train_history_[-1][\"train_loss\"], net1_stretch.train_history_[-1][\"valid_loss\"], net1_stretch.train_history_[-1][\"dur\"])\n",
    "\n",
    "\n",
    "net1_eq = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                         ('hidden', layers.DenseLayer),\n",
    "                         ('output', layers.DenseLayer),],\n",
    "                 input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                 hidden_num_units=500,  # number of units in hidden layer\n",
    "                 output_nonlinearity=None,  # output layer uses identity function\n",
    "                 output_num_units=30,  # 30 target values\n",
    "                 \n",
    "                 update=nesterov_momentum, # optimization parameters\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                 max_epochs=1000,  # we want to train this many epochs\n",
    "                 verbose=0,)\n",
    "net1_eq.fit(X_eq, y)\n",
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1_eq.train_history_[-1][\"train_loss\"], net1_eq.train_history_[-1][\"valid_loss\"], net1_eq.train_history_[-1][\"dur\"])\n",
    "\n",
    "'''\n",
    "net1_adapteq = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                         ('hidden', layers.DenseLayer),\n",
    "                         ('output', layers.DenseLayer),],\n",
    "                 input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                 hidden_num_units=500,  # number of units in hidden layer\n",
    "                 output_nonlinearity=None,  # output layer uses identity function\n",
    "                 output_num_units=30,  # 30 target values\n",
    "                 \n",
    "                 update=nesterov_momentum, # optimization parameters\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                 max_epochs=1000,  # we want to train this many epochs\n",
    "                 verbose=0,)\n",
    "net1_adapteq.fit(X_adapteq, y)\n",
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1_adapteq.train_history_[-1][\"train_loss\"], net1_adapteq.train_history_[-1][\"valid_loss\"], net1_adapteq.train_history_[-1][\"dur\"])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract training and validation loss for various models\n",
    "net1_train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "net1_valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "net1_str_train_loss = np.array([i[\"train_loss\"] for i in net1_stretch.train_history_])\n",
    "net1_str_valid_loss = np.array([i[\"valid_loss\"] for i in net1_stretch.train_history_])\n",
    "net1_eq_train_loss = np.array([i[\"train_loss\"] for i in net1_eq.train_history_])\n",
    "net1_eq_valid_loss = np.array([i[\"valid_loss\"] for i in net1_eq.train_history_])\n",
    "net1_adapteq_train_loss = np.array([i[\"train_loss\"] for i in net1_adapteq.train_history_])\n",
    "net1_adapteq_valid_loss = np.array([i[\"valid_loss\"] for i in net1_adapteq.train_history_])\n",
    "\n",
    "# Plot for performance comparision between differnt preprocesses\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(net1_train_loss, linewidth=3,  color=\"green\", linestyle=\"dashed\", label=\"Train without preprocess\")\n",
    "plt.plot(net1_valid_loss, linewidth=3,  color=\"green\", linestyle=\"solid\", label=\"Valid without preprocess\")\n",
    "plt.plot(net1_str_train_loss, linewidth=3,  color=\"red\", linestyle=\"dashed\", label=\"Train after Contrast Stretching\")\n",
    "plt.plot(net1_str_valid_loss, linewidth=3,  color=\"red\", linestyle=\"solid\", label=\"Valid after Contrast Stretching\")\n",
    "plt.plot(net1_eq_train_loss, linewidth=3,  color=\"yellow\", linestyle=\"dashed\", label=\"Train after Histogram Equalization\")\n",
    "plt.plot(net1_eq_valid_loss, linewidth=3,  color=\"yellow\", linestyle=\"solid\", label=\"Valid after Histogram Equalization\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(8*1e-4, 1e-2)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Define a listtable to hold statsitical outcomes of NNs with various preprocessing steps \n",
    "performance_table_2 = ListTable()\n",
    "performance_table_2.append(['Preprocess', 'Train_loss', 'Validation_loss', 'Duration_time(s)'])\n",
    "performance_table_2.append(['Without Preprocessing',\n",
    "                            round(net1.train_history_[-1][\"train_loss\"],5),\n",
    "                            round(net1.train_history_[-1][\"valid_loss\"],5),\n",
    "                            round(net1.train_history_[-1][\"dur\"],5)] )\n",
    "performance_table_2.append(['Contrast stretching',\n",
    "                            round(net1_stretch.train_history_[-1][\"train_loss\"],5),\n",
    "                            round(net1_stretch.train_history_[-1][\"valid_loss\"],5),\n",
    "                            round(net1_stretch.train_history_[-1][\"dur\"],5)] )    \n",
    "performance_table_2.append(['Histogram equalization',\n",
    "                            round(net1_eq.train_history_[-1][\"train_loss\"],5),\n",
    "                            round(net1_eq.train_history_[-1][\"valid_loss\"],5),\n",
    "                            round(net1_eq.train_history_[-1][\"dur\"],5)] )   \n",
    "performance_table_2  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "def idct2(xIn2Dim):\n",
    "    return fftpack.idct(fftpack.idct(xIn2Dim, norm = 'ortho').T, norm = 'ortho')\n",
    "\n",
    "def dct2(xIn, xDim = 96, yDim = 96):\n",
    "    #dctback = fftpack.idct(dct)\n",
    "\n",
    "    return fftpack.dct(fftpack.dct(xIn.reshape(xDim,yDim), norm = 'ortho').T, norm = 'ortho')\n",
    "\n",
    "def primHPF(xIn2Dim, level = 5):\n",
    "    shape0 = xIn2Dim.shape[0]\n",
    "    shape1 = xIn2Dim.shape[1]\n",
    "    \n",
    "    for i in range(shape0 - level, shape0):\n",
    "        for j in range(shape1 - level, shape1):\n",
    "            xIn2Dim[i,j] = 0\n",
    "    return xIn2Dim\n",
    "\n",
    "hpfX = np.empty_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    hpfX[i] = idct2(primHPF(dct2(X[i]))).reshape(96 * 96)\n",
    "    \n",
    "net1_hpf = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                         ('hidden', layers.DenseLayer),\n",
    "                         ('output', layers.DenseLayer),],\n",
    "                 input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                 hidden_num_units=500,  # number of units in hidden layer\n",
    "                 output_nonlinearity=None,  # output layer uses identity function\n",
    "                 output_num_units=30,  # 30 target values\n",
    "                 \n",
    "                 update=nesterov_momentum, # optimization parameters\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                 max_epochs=1000,  # we want to train this many epochs\n",
    "                 verbose=0,)\n",
    "\n",
    "net1_hpf.fit(hpfX, y)\n",
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1_hpf.train_history_[-1][\"train_loss\"], net1_hpf.train_history_[-1][\"valid_loss\"], net1_hpf.train_history_[-1][\"dur\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nolearn.lasagne import BatchIterator\n",
    "\n",
    "# randomly flip half of the images in each batch\n",
    "class FlipBatchIterator(BatchIterator):\n",
    "    flip_indices = [\n",
    "        (0, 2), (1, 3),\n",
    "        (4, 8), (5, 9), (6, 10), (7, 11),\n",
    "        (12, 16), (13, 17), (14, 18), (15, 19),\n",
    "        (22, 24), (23, 25),\n",
    "        ]\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        Xb, yb = super(FlipBatchIterator, self).transform(Xb, yb)\n",
    "\n",
    "        # Flip half of the images in this batch at random:\n",
    "        bs = Xb.shape[0]\n",
    "        indices = np.random.choice(bs, bs / 2, replace=False)\n",
    "        for i in range(0, np.int64(bs/2)):\n",
    "            X_trans = Xb[indices[i]].reshape(96, 96)\n",
    "            X_trans = X_trans[:, ::-1]\n",
    "            Xb[indices[i]] = X_trans.reshape(1, 9216)\n",
    "\n",
    "        if yb is not None:\n",
    "            # Horizontal flip of all x coordinates:\n",
    "            yb[indices, ::2] = yb[indices, ::2] * -1\n",
    "\n",
    "            # Swap places, e.g. left_eye_center_x -> right_eye_center_x\n",
    "            for a, b in self.flip_indices:\n",
    "                yb[indices, a], yb[indices, b] = (\n",
    "                    yb[indices, b], yb[indices, a])\n",
    "\n",
    "        return Xb, yb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "net1_augmentation = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                                      ('hidden', layers.DenseLayer),\n",
    "                                      ('output', layers.DenseLayer),],\n",
    "                              # layer parameters:\n",
    "                              input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                              hidden_num_units=500,  # number of units in hidden layer\n",
    "                              output_nonlinearity=None,  # output layer uses identity function\n",
    "                              output_num_units=30,  # 30 target values\n",
    "                              \n",
    "                              # optimization parameters:\n",
    "                              update=nesterov_momentum,\n",
    "                              update_learning_rate=0.01,\n",
    "                              update_momentum=0.9,\n",
    "                 \n",
    "                              regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                              batch_iterator_train=FlipBatchIterator(batch_size=128),\n",
    "                              max_epochs=1000,  # we want to train this many epocho\n",
    "                              verbose=1,)\n",
    "\n",
    "net1_augmentation.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net1_eq_augmentation = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                                      ('hidden', layers.DenseLayer),\n",
    "                                      ('output', layers.DenseLayer),],\n",
    "                              # layer parameters:\n",
    "                              input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                              hidden_num_units=500,  # number of units in hidden layer\n",
    "                              output_nonlinearity=None,  # output layer uses identity function\n",
    "                              output_num_units=30,  # 30 target values\n",
    "                              \n",
    "                              # optimization parameters:\n",
    "                              update=nesterov_momentum,\n",
    "                              update_learning_rate=0.01,\n",
    "                              update_momentum=0.9,\n",
    "                 \n",
    "                              regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                              batch_iterator_train=FlipBatchIterator(batch_size=128),\n",
    "                              max_epochs=1000,  # we want to train this many epocho\n",
    "                              verbose=1,)\n",
    "\n",
    "net1_eq_augmentation.fit(X_eq, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1_augmentation.train_history_[-1][\"train_loss\"], net1_augmentation.train_history_[-1][\"valid_loss\"], net1_augmentation.train_history_[-1][\"dur\"])\n",
    "print \"Train, valid errors and computational time of last epcho: %0.5f, %0.5f, %0.4f\" %(net1_eq_augmentation.train_history_[-1][\"train_loss\"], net1_eq_augmentation.train_history_[-1][\"valid_loss\"], net1_augmentation.train_history_[-1][\"dur\"])\n",
    "\n",
    "\n",
    "# Extract training and validation loss for various models\n",
    "net1_train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "net1_valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "net1_eq_train_loss = np.array([i[\"train_loss\"] for i in net1_eq.train_history_])\n",
    "net1_eq_valid_loss = np.array([i[\"valid_loss\"] for i in net1_eq.train_history_])\n",
    "net1_augmentation_train_loss = np.array([i[\"train_loss\"] for i in net1_augmentation.train_history_])\n",
    "net1_augmentation_valid_loss = np.array([i[\"valid_loss\"] for i in net1_augmentation.train_history_])\n",
    "net1_eq_augmentation_train_loss = np.array([i[\"train_loss\"] for i in net1_eq_augmentation.train_history_])\n",
    "net1_eq_augmentation_valid_loss = np.array([i[\"valid_loss\"] for i in net1_eq_augmentation.train_history_])\n",
    "\n",
    "# Plot for performance comparision between differnt preprocesses\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(net1_train_loss, linewidth=3,  color=\"green\", linestyle=\"dashed\", label=\"Train without preprocess\")\n",
    "plt.plot(net1_valid_loss, linewidth=3,  color=\"green\", linestyle=\"solid\", label=\"Valid without preprocess\")\n",
    "plt.plot(net1_eq_train_loss, linewidth=3,  color=\"yellow\", linestyle=\"dashed\", label=\"Train after Histogram Equalization\")\n",
    "plt.plot(net1_eq_valid_loss, linewidth=3,  color=\"yellow\", linestyle=\"solid\", label=\"Valid after Histogram Equalization\")\n",
    "plt.plot(net1_augmentation_train_loss, linewidth=3,  color=\"purple\", linestyle=\"dashed\", label=\"Train after Data Argumentation\")\n",
    "plt.plot(net1_augmentation_valid_loss, linewidth=3,  color=\"purple\", linestyle=\"solid\", label=\"Valid after Data Argumentation\")\n",
    "plt.plot(net1_eq_augmentation_train_loss, linewidth=3,  color=\"black\", linestyle=\"dashed\", label=\"Train after Data Argumentation & Histogram Equalization\")\n",
    "plt.plot(net1_eq_augmentation_valid_loss, linewidth=3,  color=\"black\", linestyle=\"solid\", label=\"Valid after Data Argumentation & Histogram Equalization\")\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(8*1e-4, 1e-2)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list of dropout rate\n",
    "dropout_rate = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "\n",
    "# Define a listtable to hold statsitical outcomes of NNs with various hiddern unit numbers \n",
    "performance_table_3 = ListTable()\n",
    "performance_table_3.append(['Dropout_rate', 'Train_loss', 'Validation_loss', 'Duration_time(s)'])\n",
    "\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "for index,num in enumerate(dropout_rate):\n",
    "    net1_dropout = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                                     ('hidden', layers.DenseLayer),\n",
    "                                     ('dropout1', layers.DropoutLayer),\n",
    "                                     ('output', layers.DenseLayer),],\n",
    "                             \n",
    "                             input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                             hidden_num_units=500,  # number of units in hidden layer\n",
    "                             output_nonlinearity=None,  # output layer uses identity function\n",
    "                             output_num_units=30,  # 30 target values\n",
    "                             \n",
    "                             update=nesterov_momentum, # optimization parameters\n",
    "                             update_learning_rate=0.01,\n",
    "                             update_momentum=0.9,\n",
    "                             \n",
    "                             regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                             max_epochs=1000,  # we want to train this many epochs\n",
    "                             verbose=0,)\n",
    "    net1_dropout.dropout1_p = num\n",
    "    net1_dropout.fit(X, y)\n",
    "    \n",
    "    train_loss = np.array([i[\"train_loss\"] for i in net1_dropout.train_history_])\n",
    "    valid_loss = np.array([i[\"valid_loss\"] for i in net1_dropout.train_history_])\n",
    "    print \"Dropout Rate is: %.2f\" %net1_dropout.dropout1_p \n",
    "    print \"Train, valid errors and computational time of last epcho: %0.4f, %0.4f, %0.4f\" %(net1_dropout.train_history_[-1][\"train_loss\"], net1_dropout.train_history_[-1][\"valid_loss\"], net1_dropout.train_history_[-1][\"dur\"])\n",
    "    performance_table_3.append([net1_dropout.dropout1_p,\n",
    "                                round(net1_dropout.train_history_[-1][\"train_loss\"],5),\n",
    "                                round(net1_dropout.train_history_[-1][\"valid_loss\"],5),\n",
    "                                round(net1_dropout.train_history_[-1][\"dur\"],5)] )\n",
    "    \n",
    "    subfig = fig.add_subplot(2, 5, index+1, xticks=[], yticks=[])\n",
    "    plt.plot(train_loss, linewidth=3, color=\"blue\", linestyle=\"dashed\", label=\"train\")\n",
    "    plt.plot(valid_loss, linewidth=3, color=\"blue\", linestyle=\"solid\", label=\"valid\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.ylim(5*1e-4, 1e-2)\n",
    "    plt.xlim(0,1000)\n",
    "    plt.yscale(\"log\")\n",
    "    subfig.set_title('Dropout Rate: ' + str(num))\n",
    "\n",
    "plt.show()\n",
    "performance_table_3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net1_dropout = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                                 ('hidden', layers.DenseLayer),\n",
    "                                 ('dropout1', layers.DropoutLayer),\n",
    "                                 ('output', layers.DenseLayer),],\n",
    "                         \n",
    "                         input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                         hidden_num_units=500,  # number of units in hidden layer\n",
    "                         output_nonlinearity=None,  # output layer uses identity function\n",
    "                         output_num_units=30,  # 30 target values\n",
    "                         dropout1_p=0.1,\n",
    "                 \n",
    "                         update=nesterov_momentum, # optimization parameters\n",
    "                         update_learning_rate=0.01,\n",
    "                         update_momentum=0.9,\n",
    "                         \n",
    "                         regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                         max_epochs=1000,  # we want to train this many epochs\n",
    "                         verbose=1,)\n",
    "\n",
    "\n",
    "net1_dropout.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "#from lasagne.nonlinearities import softmax\n",
    "#from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne import regularization\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "\n",
    "net1_l2norm = NeuralNet(layers=[('input', layers.InputLayer), # three layers\n",
    "                                ('hidden', layers.DenseLayer),\n",
    "                                ('output', layers.DenseLayer),],\n",
    "                         \n",
    "                        input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "                        hidden_num_units=500,  # number of units in hidden layer\n",
    "                        output_nonlinearity=None,  # output layer uses identity function\n",
    "                        output_num_units=30,  # 30 target values\n",
    "                          \n",
    "                        loss = lasagne.objectives.mse + 0.0001*lasagne.regularization.l2(output),\n",
    "                        \n",
    "                        update=nesterov_momentum, # optimization parameters\n",
    "                        update_learning_rate=0.01,\n",
    "                        update_momentum=0.9,\n",
    "                         \n",
    "                        regression=True,  # flag to indicate we're dealing with regression problem\n",
    "                        max_epochs=1000,  # we want to train this many epochs\n",
    "                        verbose=1,)\n",
    "\n",
    "\n",
    "net1_l2norm.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nolearn.lasagne import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
